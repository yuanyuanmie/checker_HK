{
  "1": "### Section 1.2: Minimum requirements for use of IRB approach\nMinimum requirements for use of IRB approach\n\n1.2.1: Part 6 and Schedule 2 of the BCR set out the capital adequacy framework and minimum requirements for an AI to use the IRB approach to calculate its credit risk exposures within one or more IRB adoption classes.  AIs are therefore advised to read this module in conjunction with the BCR.  In case of any discrepancy between the BCR and this module, the former shall prevail.\n\n1.2.2: In addition, the module should also be read in conjunction with Q&As:IV and other relevant documents issued by the HKMA.\n\n1.2.3: An AI may submit an application under §8(1) of the BCR to use the IRB approach to calculate its credit risk for non-securitization exposures for one or more IRB adoption classes.  The MA may grant approval to the AI under§8(2)(a), subject to any conditions that the MA thinks proper in any particular case(see §33A of the BCR), to use the IRB approach if the AI demonstrates to the satisfaction of the MA that the minimum requirements specified in Schedule 2 to the BCR applicable to the AI are met.\n\n1.2.4: In practice, based on an AI’s indication of its intent to make the application, the HKMA will arrange to undertake the IRB recognition process to assess the AI’s eligibility to use the IRB approach, including whether the AI’s overall credit risk management practices are consistent with the relevant provisions in the BCR, and the applicable guidelines and sound practices issued by the Basel Committee and the HKMA.  Should approval be granted to the AI, the HKMA will conduct reviews of the AI from time to time to ascertain the AI’s continuous compliance with the applicable HKMA requirements.\n\n1.2.5: Where an AI adopting the IRB approach is not in full compliance with the applicable HKMA requirements, or it has contravened a condition attached to its IRB approval, the MA may take one or more of the measures set out in§10(5) of the BCR1.  These include a requirement for the AI to: (i)    use the STC approach(instead of the IRB approach) to calculate the credit risk for the concerned exposures; (ii)   submit to the MA a remedial plan and implement that plan; (iii)   reduce its credit exposures; (iv)   hold additional capital under the supervisory review process; and/or\n\n### Section 1.3: Scope\nScope\n\n1.3.1: This module sets out: (i)   the HKMA’s approach to the validation of rating systems of AIs for the purposes of using the IRB approach; and (ii) the HKMA’s expectations for AIs using(or intending to use) the IRB approach by elaborating on the applicable HKMA requirements having regard to the publications of the Basel Committee, industry practices and the HKMA’s experience.\n\n1.3.2: The requirements and supervisory expectations set out in this module apply to all rating systems for use under various IRB calculation approaches stipulated in Table 17in §147(1) of the BCR, including rating systems that are obtained from a third-party vendor as well as group-wide rating systems(i.e. rating systems that have been used by a bank incorporated outside Hong Kong and this bank is a member of a group of companies of which the AI is also a member).\n\n1.3.3: The scope of the applicable HKMA requirements, and the scope and intensity of the HKMA’s IRB recognition process and ongoing supervision, will depend on the specific circumstances of individual AIs, for instance, whether it is an application for the initial use of the IRB approach or for modifying an approved rating system, the nature and scale of the exposures being covered, and the IRB calculation approach being involved.\n\n### Section 5.2: Transparency\nTransparency\n\n5.2.1: AIs’ rating systems should be transparent to enable third parties, such as rating system reviewers, internal or external auditors, and the HKMA, to understand the design, operations and accuracy of the rating systems, and to evaluate whether the systems are performing as intended.  Transparency is an ongoing requirement and should    be    achieved   through   comprehensive documentation with regular and timely reviews, and updates as appropriate(e.g. as and when modifications are made to the rating systems).  This underlies the minimum requirements stipulated in §1(e) of Schedule 2to the BCR.\n\n5.2.2: An AI should document in writing the design of its rating systems and related operations as evidence of its compliance with the applicable HKMA requirements.\n\n5.2.3: The AI’s documentation should provide a description of the overarching design of the rating systems, including: (i)   the purpose of the rating systems; (ii)  portfolio differentiation; and (iii) the rating approach(i.e. how quickly ratings are expected to migrate in response to economic cycles) and implications for the AI’s capital planning process14.\n\n5.2.4: Rating criteria and definitions should be clearly documented. These include: (i)   the relationship between obligor grades(or pools for retail exposures) in terms of the level of risk each grade(or pool) implies, and the risk of each grade(or pool) in terms of both a description of the probability of default typical for obligors assigned the grade(or pool) and the criteria used to\n\n5.2.5: Documentation of the rating process and rating system operations should include the following: (i)   the organisation of rating assignment/approval; (ii)  responsibilities of parties that rate obligors and facilities; (iii) parties that have the authority to approve ratings and that have the authority to approve exceptions(including overrides); (iv) situations where exceptions and overrides can be approved and the procedures for such approval; (v) the rating criteria and procedures(including the frequency of rating reviews); (vi) the process and procedures for updating obligor and facility information; and (vii) the rationale and criteria for assigning obligors(or facilities) to a particular rating system if multiple rating systems are used.\n\n5.2.6: In respect of internal control structure, the documentation should be able to demonstrate that: (i)   the Board and senior management have adequate oversight of the rating systems and rating process; (ii)  independence of the rating assignment/approval process is achieved and ensured; (iii) there are proper audit trails on the history of major changes to the rating process and criteria, in particular to support identification of changes made to the rating process and criteria subsequent to the last supervisory review15; and (iv) there are established procedures(including frequency, parties responsible and reporting of results) and performance standards for reviewing the rating systems in respect of rating accuracy, rating criteria and rating processes in order to determine whether they remain fully applicable to the current portfolio and to external conditions, and that these procedures are adhered to.\n\n5.2.7: Where judgement is used in the rating process, how personal experience and subjective assessment are deployed is less transparent.  AIs should offset this shortcoming by applying greater independence in the rating approval process and an enhanced rating system review.\n\n5.2.8: Where an AI employs a statistical model in the rating process and/or estimation of the credit risk components, its documentation should include: (i)   the theory, assumptions and/or mathematical and empirical   basis   of    the   assignment   of obligors/facilities to grades(or pools for retail exposures) and estimation of the associated credit risk components, and the sources of data used to develop the model;\n\n5.2.9: Use of a model obtained from a third-party vendor that claims proprietary technology is not a justification for exemption from documentation or any other applicable HKMA requirements.  The burden is on the vendor and the AI to satisfy the applicable HKMA requirements.",
  "2": "### Section 1.2: Minimum requirements for use of IRB approach\nMinimum requirements for use of IRB approach\n\n1.2.1: Part 6 and Schedule 2 of the BCR set out the capital adequacy framework and minimum requirements for an AI to use the IRB approach to calculate its credit risk exposures within one or more IRB adoption classes.  AIs are therefore advised to read this module in conjunction with the BCR.  In case of any discrepancy between the BCR and this module, the former shall prevail.\n\n1.2.2: In addition, the module should also be read in conjunction with Q&As:IV and other relevant documents issued by the HKMA.\n\n1.2.3: An AI may submit an application under §8(1) of the BCR to use the IRB approach to calculate its credit risk for non-securitization exposures for one or more IRB adoption classes.  The MA may grant approval to the AI under§8(2)(a), subject to any conditions that the MA thinks proper in any particular case(see §33A of the BCR), to use the IRB approach if the AI demonstrates to the satisfaction of the MA that the minimum requirements specified in Schedule 2 to the BCR applicable to the AI are met.\n\n1.2.4: In practice, based on an AI’s indication of its intent to make the application, the HKMA will arrange to undertake the IRB recognition process to assess the AI’s eligibility to use the IRB approach, including whether the AI’s overall credit risk management practices are consistent with the relevant provisions in the BCR, and the applicable guidelines and sound practices issued by the Basel Committee and the HKMA.  Should approval be granted to the AI, the HKMA will conduct reviews of the AI from time to time to ascertain the AI’s continuous compliance with the applicable HKMA requirements.\n\n1.2.5: Where an AI adopting the IRB approach is not in full compliance with the applicable HKMA requirements, or it has contravened a condition attached to its IRB approval, the MA may take one or more of the measures set out in§10(5) of the BCR1.  These include a requirement for the AI to: (i)    use the STC approach(instead of the IRB approach) to calculate the credit risk for the concerned exposures; (ii)   submit to the MA a remedial plan and implement that plan; (iii)   reduce its credit exposures; (iv)   hold additional capital under the supervisory review process; and/or\n\n### Section 1.3: Scope\nScope\n\n1.3.1: This module sets out: (i)   the HKMA’s approach to the validation of rating systems of AIs for the purposes of using the IRB approach; and (ii) the HKMA’s expectations for AIs using(or intending to use) the IRB approach by elaborating on the applicable HKMA requirements having regard to the publications of the Basel Committee, industry practices and the HKMA’s experience.\n\n1.3.2: The requirements and supervisory expectations set out in this module apply to all rating systems for use under various IRB calculation approaches stipulated in Table 17in §147(1) of the BCR, including rating systems that are obtained from a third-party vendor as well as group-wide rating systems(i.e. rating systems that have been used by a bank incorporated outside Hong Kong and this bank is a member of a group of companies of which the AI is also a member).\n\n1.3.3: The scope of the applicable HKMA requirements, and the scope and intensity of the HKMA’s IRB recognition process and ongoing supervision, will depend on the specific circumstances of individual AIs, for instance, whether it is an application for the initial use of the IRB approach or for modifying an approved rating system, the nature and scale of the exposures being covered, and the IRB calculation approach being involved.\n\n### Section 2.1: The HKMA’s approach to validation adheres to the principles promulgated by the Basel Committee 2(“Basel IRB validation principles”) as follows: (i)    Validation is fundamentally about assessing the predictive ability of a bank’s risk estimates and the use of ratings in credit processes; (ii)   The bank has primary responsibility for validation; (iii)   Validation is an iterative process; (iv)   There is no single validation method; (v)   Validation should encompass both quantitative and qualitative elements; and (vi)   Validation processes and outcomes should be subject to independent review.\nThe HKMA’s approach to validation adheres to the principles promulgated by the Basel Committee 2(“Basel IRB validation principles”) as follows: (i)    Validation is fundamentally about assessing the predictive ability of a bank’s risk estimates and the use of ratings in credit processes; (ii)   The bank has primary responsibility for validation; (iii)   Validation is an iterative process; (iv)   There is no single validation method; (v)   Validation should encompass both quantitative and qualitative elements; and (vi)   Validation processes and outcomes should be subject to independent review.",
  "3": "### Section 3.1.3: 3.1 Subsection\nAn important aspect in assessing a rating system’s logic and conceptual soundness is its economic plausibility. The risk factors that are included in the rating system should be well grounded in the relevant economic and financial theory and in established empirical relationships, rather than spurious relationships which are purely driven by the underlying data.  AIs should be able to provide valid explanations on why particular risk factors are included in the rating system.  Where possible, AIs should assess the discriminatory power and predictive ability of individual risk factors, and analyse how individual factors behave and interact with other factors in the multivariate context in order to justify their inclusion.  Other important aspects include the relevancy of data used to develop and calibrate the rating system(e.g. whether the data are representative of the population of the AIs’ actual obligors or facilities), and whether the criteria for system screening in the developmental stage are well supported in theory and evidence and are applied consistently.\n\n### Section 3.1.4: 3.1 Subsection\nAIs should be able to demonstrate that their rating systems and the associated credit risk component estimates take into account all relevant and material information.  This includes, amongst others, the AIs’lending practices or processes for pursuing recoveries, as well as any changes to such practices or processes, which may affect the accuracy of their rating systems and the associated credit risk component estimates.\n\n### Section 3.5.1: 3.5 Subsection\nAnother important factor affecting an AI’s eligibility for using the IRB approach is whether the AI has a robust system in place to validate the accuracy and consistency of its rating systems, processes, and the associated credit risk component estimates, and whether the validation process enables the AI to assess the performance of its rating systems consistently and meaningfully.",
  "4": "### Section 3.1.1: 3.1 Subsection\nRating systems can be generally classified into two broad types, namely model-based and judgement-based.  The former is a mechanical process, relying primarily on quantitative techniques such as credit scoring models, statistical default prediction models and specified objective financial analysis.  The latter relies primarily on the personal experience and subjective judgement of credit officers5.\n\n### Section 3.1.5: 3.1 Subsection\nWhere human judgement forms part or all of the inputs to a rating system, or where judgement is combined with outputs of a model in determining the final ratings, there should be written guidelines on how the judgement and combination are exercised6.  Such guidelines should set out the risk factors that need to be considered and how they should be considered in the rating process, including the relative importance of these factors.  AIs should have a robust monitoring and rating approval process to ensure that the judgement is properly, consistently and prudently exercised, and adheres to the established guidelines.\n\n### Section 3.5.3: 3.5 Subsection\nAIs should also use other quantitative validation tools and comparisons with relevant external data sources(see section 11 on benchmarking). The analysis should be based on data that are appropriate to the portfolio, are updated regularly, and cover a relevant observation period.  AIs’ assessments of the performance of their rating systems should be based on long data histories, covering a range of economic conditions, and ideally one or more complete economic cycles.",
  "5": "### Section 3.1.4: 3.1 Subsection\nAIs should be able to demonstrate that their rating systems and the associated credit risk component estimates take into account all relevant and material information.  This includes, amongst others, the AIs’lending practices or processes for pursuing recoveries, as well as any changes to such practices or processes, which may affect the accuracy of their rating systems and the associated credit risk component estimates.\n\n### Section 3.5.6: 3.5 Subsection\nAIs should have well-articulated internal standards for situations where deviations in realized values of the credit risk components from expectations become significant enough to call the validity of the estimates into question. These standards should take account of economic cycles and similar systematic variability in the AIs’ default and loss experiences.  AIs should put in place a framework for revising the credit risk component estimates upward to reflect their default and loss experiences when realized values continue to be higher than the expected values.\n\n### Section 8.1.2: 8.1 Subsection\nAn AI’s own workout and collection approach as well as the relevant expertise can significantly influence its recovery of defaulted exposures and therefore should be reflected in its LGD estimates as appropriate.  However, AIs should take account of such approach and expertise conservatively(e.g. until there is sufficient empirical evidence showing the positive impact of the expertise).",
  "6": "### Section 3.1.6: 3.1 Subsection\nIn relation to §159(1)(d), §161(1)(e), §164(4)(f),§177(1)(e), §178(1)(g) and §180(3)(b) of the BCR regarding the length of historical data period for estimation of the credit risk components, AIs should consider and use data with the longest period as appropriate irrespective of the data sources(external, internal, pooled data sources, or any combination of the three) if such data are relevant and material.  The data should include a representative mix of good and bad years of the economic cycle relevant to the portfolio.",
  "7": "### Section 6.2: Management oversight and control\nManagement oversight and control\n\n6.2.1: Senior management of an AI have the responsibility for establishing and maintaining a consistent standard of sound practices for data management across the AI.  In particular, senior management are responsible for:\n\n6.2.2: Where data management-related activities are performed on behalf of the AI by another entity in the same banking group, such as an office outside Hong Kong, the management of the AI are responsible for ensuring that the standards of data management employed by the group entity are consistent with the applicable HKMA requirements, and that the respective responsibilities of the entity and the AI are documented(e.g. policies, procedures or service agreements) and properly implemented.\n\n### Section 6.3: IT infrastructure and data architecture\nIT infrastructure and data architecture\n\n6.3.1: An AI should have an adequate IT infrastructure(e.g. data warehouse or data mart) in place to support the management of data.  In particular, AIs should store data in electronic format so as to allow timely retrieval for analysis and validation of rating systems.  The infrastructure should also support comprehensive data quality control measures including data validation and error detection, data cleansing, reconciliation and exceptions reporting.\n\n6.3.2: AIs’ data architecture should be scalable, secure and stable21.  Scalability ensures that growing needs due to lengthening data history and business expansion can be met.  AIs should test systems’ security and stability in the development of data architecture and IT systems.  The HKMA expects AIs to have policies, standards and measures, including audit trails, in place to control access to the data.  AIs should also have complete back-up, recovery and contingency planning to protect data integrity in the event of emergency or disaster22.\n\n6.3.3: AIs are expected to perform adequate user acceptance tests to ascertain that new or changes to IT systems(including those arising from adoption of a new rating system or modifications to an existing rating system) will perform as intended.\n\n### Section 6.4: Data collection, storage, retrieval and deletion\nData collection, storage, retrieval and deletion\n\n6.4.1: AIs should have clear and documented policies, standards(including IT standards) and procedures regarding the collection and maintenance of data in practice, such that data availability can be ensured over time to meet the anticipated demands in the medium and long run, and the data stored include sufficient details so as to enable the AIs to comply with the applicable HKMA requirements in relation to data management.\n\n6.4.2: Data should be updated at least annually or more frequently as required in accordance with the relevant minimum updating requirement for estimation of the credit risk components23.  AIs should be able to demonstrate that their procedures to ensure that the frequency with which data items are updated are sufficient to reflect the risk inherent in their current portfolios.  For example, data for obligors with higher default risk or delinquent exposures should be subject to higher updating frequency.\n\n6.4.3: The HKMA also expects AIs to: (i)   establish clear and comprehensive documentation for data definition, collection and aggregation, including data sources, updating and aggregation routines; (ii)  establish standards and conduct relevant tests on the accuracy, completeness, timeliness and reliability of data; (iii) ensure that data collected have the scope, depth and reliability to support the operations of rating systems, overrides, back-testing, regulatory capital calculation   and   relevant  management  and regulatory reporting; (iv) in cases where the necessary data items are absent in the collection process(i.e. data gaps), identify and document such gaps, specify the interim solutions in respect of the rating assignment and risk quantification processes and set up a plan to fill the gaps;\n\n### Section 6.7: Reconciliation\nReconciliation\n\n6.7.1: The HKMA expects AIs to conduct reconciliation, where possible, between accounting data and the data used in the risk quantification process under the IRB approach. This would require AIs to identify from the risk quantification data set those data items that can be reconciled with accounting data, and establish the procedures for doing so. Theguidance set out in TM-G-2 “Business Continuity Planning” is applicable here.\n\n6.7.2: Both an AI’s rating systems and its accounting systems take data inputs and transform them into data outputs. Therefore, reconciliation between these systems may focus on inputs, outputs(e.g. expected loss under the IRB approach and relevant accounting provisions) or both.  At a minimum, AIs should conduct reconciliation on data inputs.\n\n6.7.3: AIs should document the reconciliation process and results(i.e. the amount of the difference between the two data sets), as well as the explanations for why and how the difference arises.  The explanations should be sufficiently detailed(e.g. how much of the difference is attributable to non-identical treatments for regulatory capital calculation and accounting purposes) and supported by sufficient evidence to facilitate internal audit function in verifying enterprise-wide consistency in the use of data and assessing data accuracy, completeness and appropriateness.\n\n6.7.4: AIs should document the treatment for non-reconciled items(i.e. the amount of difference that cannot be fully explained).  In addition, as non-reconciliation may be an indication of deficiency in data quality, AIs should establish standards to address this, and enhance their data management process and apply conservatism in regulatory   capital   calculation   when   there   are discrepancies.  The HKMA may not approve an AI’s rating systems if, in its opinion, the discrepancies are of such significance as to cast doubt on the reliability of the systems.",
  "8": "### Section 6.8: Data quality assessment\nData quality assessment\n\n6.8.1: In addition to qualitative assessments on the adequacy of the aspects described in subsections 6.2 to 6.7, the HKMA expects AIs to apply quantitative measures in assessing data accuracy(e.g. error rates in sample checking of data accuracy), completeness(e.g. proportion of observations with missing data) and timeliness(e.g. proportion of data updated later than scheduled).\n\n6.8.2: The data quality assessment should be included as part of the independent review and validation of the rating assignment and risk quantification processes.  While the reviewers may either be internal or external parties, they must not be accountable for the work being reviewed.\n\n6.8.3: The data quality assessment should be conducted at least annually, matching the minimum frequency of validation of rating systems by independent validation unit(s) and the review of adherence to all applicable HKMA requirements by internal audit function.  In addition, AIs are expected to track how the previously identified deficiencies, if any, have been treated and addressed.\n\n6.8.4: The methods employed and analyses conducted in the assessment should be fully documented.  The assessment results should be reported to senior management, and further investigation and follow-up actions should be fully documented.\n\n6.8.5: To facilitate quality assessment and identification of problems, AIs should ensure that there are clear audit trails on data(information on where the data are collected, how they are processed and stored, and used in the rating assignment and risk quantification processes etc.).\n\n### Section 6.9: Use of external and pooled data\nUse of external and pooled data\n\n6.9.1: AIs that use external or pooled data in rating system development and validation, rating assignment and/or risk quantification processes must be able to demonstrate that the data are applicable and relevant to the portfolio to which they are being applied.  AIs should be able to demonstrate that data definitions are consistent between the external or pooled data, and AIs’ internal portfolio data, and that distributions of the key risk characteristics(e.g. industry and company size) are similar.\n\n6.9.2: AIs should be able to demonstrate that the arrangements for data management by third-party vendors in relation to external or pooled data used by the AIs meet the same standards required for data management by the AIs.  In addition, AIs should have policies and procedures in place to assess and control the risk arising from the use of external or pooled data.  In particular, AIs are expected to: (i)   understand how the third-party vendors collect the data; (ii)  understand the quality control programmes used by the third-party vendors and evaluate the adequacy thereof; (iii) establish data cleansing procedures for the external or pooled data; (iv) check the external or pooled data against multiple sources no less than once every 12 months to ensure the accuracy, completeness and timeliness of data; and (v) conduct reviews no less than once every 12 months to assess the appropriateness of continuing the use of the external or pooled data.\n\n6.9.3: The process of managing the use of external or pooled data, including the activities described above, should be documented and subject to review by the AIs’ internal audit function.\n\n6.9.4: When outsourcing activities are involved in the data management process, AIs should follow the guidance set out in SA-2 “Outsourcing” and section 7 of TM-G-1“General Principles for Technology Risk Management”.",
  "9": "### Section 6.10: Statistical issues\nStatistical issues\n\n6.10.1: Where AIs use statistical techniques(e.g. sampling, smoothing and sample truncation to remove outlying observations) in the preparation of the development and validation data sets, and in the operations of rating systems, their application should be justified and based on sound scientific methods.  AIs should be able to demonstrate a full understanding of the properties and limitations of the statistical techniques they use, and the applicability of these techniques to different types of data.\n\n6.10.2: AIs should be able to demonstrate that the occurrences of missing data are random and that they do not have systematic relationships with default events or credit losses.  Where it is necessary to remove observations with missing data, AIs should provide sound justifications, as these observations may contain important information on default events or credit losses.  The HKMA does not normally consider that an AI has a robust rating system if a large number of observations with missing data have been removed from the data sets used in the development, validation and operations of the system.\n\n### Section 6.9: Use of external and pooled data\nUse of external and pooled data\n\n6.9.1: AIs that use external or pooled data in rating system development and validation, rating assignment and/or risk quantification processes must be able to demonstrate that the data are applicable and relevant to the portfolio to which they are being applied.  AIs should be able to demonstrate that data definitions are consistent between the external or pooled data, and AIs’ internal portfolio data, and that distributions of the key risk characteristics(e.g. industry and company size) are similar.\n\n6.9.2: AIs should be able to demonstrate that the arrangements for data management by third-party vendors in relation to external or pooled data used by the AIs meet the same standards required for data management by the AIs.  In addition, AIs should have policies and procedures in place to assess and control the risk arising from the use of external or pooled data.  In particular, AIs are expected to: (i)   understand how the third-party vendors collect the data; (ii)  understand the quality control programmes used by the third-party vendors and evaluate the adequacy thereof; (iii) establish data cleansing procedures for the external or pooled data; (iv) check the external or pooled data against multiple sources no less than once every 12 months to ensure the accuracy, completeness and timeliness of data; and (v) conduct reviews no less than once every 12 months to assess the appropriateness of continuing the use of the external or pooled data.\n\n6.9.3: The process of managing the use of external or pooled data, including the activities described above, should be documented and subject to review by the AIs’ internal audit function.\n\n6.9.4: When outsourcing activities are involved in the data management process, AIs should follow the guidance set out in SA-2 “Outsourcing” and section 7 of TM-G-1“General Principles for Technology Risk Management”.",
  "10": "### Section 6.10: Statistical issues\nStatistical issues\n\n6.10.1: Where AIs use statistical techniques(e.g. sampling, smoothing and sample truncation to remove outlying observations) in the preparation of the development and validation data sets, and in the operations of rating systems, their application should be justified and based on sound scientific methods.  AIs should be able to demonstrate a full understanding of the properties and limitations of the statistical techniques they use, and the applicability of these techniques to different types of data.\n\n6.10.2: AIs should be able to demonstrate that the occurrences of missing data are random and that they do not have systematic relationships with default events or credit losses.  Where it is necessary to remove observations with missing data, AIs should provide sound justifications, as these observations may contain important information on default events or credit losses.  The HKMA does not normally consider that an AI has a robust rating system if a large number of observations with missing data have been removed from the data sets used in the development, validation and operations of the system.\n\n### Section 7.2.3: 7.2 Subsection\nThe HKMA expects AIs to validate both the discriminatory power and calibration of their rating systems no less than once every 12 months.  Such validations should be conducted based on the definition of default stipulated in§149 of the BCR, notwithstanding any alternative definitions for default AIs may employ for their own internal risk management purposes.",
  "11": "### Section 7.1: Supervisory expectations for estimation of PD Corporate, sovereign and bank exposures\nSupervisory expectations for estimation of PD Corporate, sovereign and bank exposures\n\n7.1.1: For the purposes of §159(2)(b) of the BCR regarding the use of multiple techniques in PD estimation, AIs should recognize the importance of judgments in combining the results of different techniques and in making adjustments for limitations of the techniques and information. Mechanical application of a technique without supporting analysis is not acceptable.\n\n7.1.2: AIs may use one or more of the three techniques in PD estimation, namely internal default experience, mapping to external data and statistical default prediction models.  For all of them, AIs must estimate a PD for each rating grade based on the observed historical average one-year default rate that is a simple average based on number of obligors(count weighted).  Other weighting approaches, such as EAD weighting, are not permitted.\n\n7.1.3: If an AI uses data on internal default experience for the estimation of PD, it should: (i)   demonstrate in its analysis that the estimates are reflective of underwriting standards and of any differences in the rating system that generated the data and the current rating system; (ii)  add a greater margin of conservatism in its PD estimates where only limited data are available, or where underwriting standards or rating systems have changed; and (iii) in case of using pooled data across institutions, demonstrate that the rating systems and criteria of other institutions contributing to the pooled data are comparable with its own.\n\n7.1.4: If an AI associates or maps it internal grades to the scale used by an external credit assessment institution or a similar institution(“external institution”), and attributes the default rates observed for the external institution's grades to its grades, the AI should: (i)   perform the mappings based on a comparison of its internal rating criteria to the criteria used by the external institution, and on a comparison of the internal and external ratings of any common borrowers; (ii)  avoid any biases or inconsistencies in the mapping approach or underlying data; (iii) ascertain that the external institution’s criteria underlying the data used for quantification are oriented to the default risk of borrowers and do not reflect transaction characteristics; (iv) include a comparison of the default definitions used having regard to the definition stipulated in §149 of the BCR, and an analysis of the implication for the PD assigned to its internal grades;\n\n7.1.5: If an AI uses a statistical default prediction model for PD estimation, it may use a simple average of the default-probability estimates generated by the model for individual obligors in a given grade for the calculation of credit risk of exposures to such obligors. Estimation techniques for retail exposures\n\n7.1.6: In general, AIs are expected to estimate the PD of their retail exposures based on their internal default experience or statistical default prediction models, and paragraphs 7.1.3 and 7.1.5 are applicable as appropriate.\n\n### Section 7.2: Overview on validation of PD\nOverview on validation of PD\n\n7.2.1: There are two key stages in the validation of PD:validation of the risk differentiation capability(i.e. discriminatory power) of a rating system and validation of the calibration of a rating system(accuracy of the PD quantification).  For each stage, the HKMA expects AIs to be able to demonstrate that they employ one or more of the quantitative techniques listed in subsections 7.3 and7.4 respectively(see Annexes A and B for details).  The procedures and assumptions used in applying the techniques must be documented and consistently applied.\n\n7.2.2: If an AI intends to use techniques not included in subsections 7.3 and 7.4, it should observe the requirements set out in paragraph 3.5.4.\n\n7.2.3: The HKMA expects AIs to validate both the discriminatory power and calibration of their rating systems no less than once every 12 months.  Such validations should be conducted based on the definition of default stipulated in§149 of the BCR, notwithstanding any alternative definitions for default AIs may employ for their own internal risk management purposes.\n\n### Section 7.3: Validation of discriminatory power\nValidation of discriminatory power\n\n7.3.1: The HKMA expects AIs to demonstrate that they use one or more of the following methodologies in assessing the discriminatory power of a rating system: (i)   Cumulative Accuracy Profile(“CAP”) and its summary index, the Accuracy Ratio(“AR”); (ii) Receiver Operating Characteristic(“ROC”) and its summary indices, the ROC measure, the Pietra Index and Kolmogorov-Smirnov(“KS”) test statistic; (iii) Bayesian error rate(“BER”); (iv) Conditional entropy, Kullback-Leibler distance, and Conditional Information Entropy Ratio(“CIER”); (v) Information value(“IV”); (vi) Kendall’s t and Somers’ D(for shadow ratings); (vii) Brier score(“BS”); and (viii) Divergence. Stability analysis\n\n7.3.2: The HKMA expects AIs to demonstrate that their rating systems exhibit stable discriminatory power.  Therefore, in addition to in-sample validation, AIs should be able to demonstrate their rating systems’ discriminatory power on an out-of-sample and out-of-time basis.  This is to ensure that the discriminatory power is stable on data sets that are cross-sectionally or temporally independent of,but structurally similar26 to, the development data set.  If out-of-sample and out-of-time validations cannot be conducted due to data constraints, AIs are expected to employ statistical techniques such as k-fold cross validation27 or bootstrapping28 for this purpose.  When an AI uses these statistical techniques, it should be able to provide the rationale for using these techniques and demonstrate the appropriateness of the chosen techniques, and understand the limitations, if any, of these techniques.\n\n7.3.3: Consistent with the expectations set out in paragraph3.5.7, the HKMA expects AIs to establish internal standards for assessing the discriminatory power of their rating systems.  Breaches of these standards, together with the associated responses, should be fully documented.  The HKMA expects to see a range of responses from an increase in validation frequency to redevelopment of the rating systems, depending on the results of the assessments.\n\n7.3.4: The HKMA expects an AI’s internal standards for its rating systems’ discriminatory power, and its responses to breaches of these standards, to be commensurate with the potential impact on the AI’s financial soundness of a failure of its rating systems to discriminate adequately between defaulting and non-defaulting obligors.\n\n### Section 7.4: Validation of calibration\nValidation of calibration\n\n7.4.1: The HKMA expects AIs to demonstrate the use of either or both of the following methodologies in assessing the accuracy of its estimates of PD: (i)   Binomial test with assumption of independent default events; and (ii) Binomial test with assumption of non-zero default correlation. AIs may also assess the accuracy of PD estimates for multiple rating grades(or pools for retail exposures) by using other methodologies such as Chi-square test. However, the AIs should also adopt methodologies which are aimed for assessing the accuracy of the PD estimates for individual grades(or pools) in order to satisfy the requirement set out in paragraph 3.5.2. Establishment of internal tolerance limits and responses\n\n7.4.2: For the purposes of paragraph 3.5.7, AIs may construct the tolerance limits(and the associated policy on remedial actions) around the confidence levels used in the tests in paragraph 7.4.1 29.  However, AIs should refrain from using this approach if the resulting tolerance limits are excessively high as compared to the PD estimates(i.e. the realized default rates will need to be very high as compared to the PD estimates in order to constitute a breach of the internal standards).\n\n### Section Annex A: Section Annex A\nQuantitative techniques in validating discriminatory power\n\n### Section Annex A: Section Annex A\nQuantitative techniques in validating discriminatory power A1.   Generating the data set for validation A1.1 In order to generate the data set for validation, an AI needs to define two cut-off dates with an interval of at least 12 months(the assessment horizon or observation period).  The rating information(obligor grade, credit score or pool for retail exposures) on a predefined set of obligors as of the earlier cut-off date(the observation point) is collected.  Then the associated performance information(i.e. default or not) on these obligors as of the later cut-off date is added. A1.2 The set of obligors chosen as the validation data set determines whether the validation is in-sample, out-of-sample or out-of-time.  Regardless of the type of validation, the validation data set should be structurally similar to the AI’s actual portfolio in terms of the obligors’ characteristics such as industry, company size, residency and income. A1.3 Information on obligors that have defaulted before the first cut-off date cannot be used.  Cases for which the loans were properly repaid during the assessment horizon should be included and are classified as “non-default”.  Cases for which no rating information as of the first cut-off date is available(e.g. new accounts) cannot be included in the sample. Updated rating information on the obligors between the cut-off dates cannot be used.  Figure A1 depicts how a validation data set is generated. A1.4 Based on the information collected, the distributions of defaulters and non-defaulters as per obligor grade(or score or range of scores, or pool for retail exposures) can be obtained and used for validation. A1.5 Data of different pairs of cut-off dates can be pooled for validation.  This is especially necessary when the sample size within each pair of cut-off dates is not large enough.  But the resulting measures will be an indication of the average discriminatory power over the relevant period. A1.6 Out-of-sample and out-of-time validation to a certain extent can verify the stability of a rating system.  Besides, an AI can generate sub-samples from the validation data set or use various assessment horizons(e.g. two years), and check whether the discriminatory power of a rating system is stable across the sub-samples or different assessment horizons.",
  "12": "### Section 8.1: Supervisory expectations for estimation of LGD\nSupervisory expectations for estimation of LGD\n\n8.1.1: The definition of loss for LGD estimation is economic loss, as reflected in the requirement set out in §161(2)(b) and§178(2)(b) of the BCR.  AIs should not simply use accounting loss as their LGD estimates, although they should be able to compare accounting and economic losses. Recognition of recovery and collateral\n\n8.1.2: An AI’s own workout and collection approach as well as the relevant expertise can significantly influence its recovery of defaulted exposures and therefore should be reflected in its LGD estimates as appropriate.  However, AIs should take account of such approach and expertise conservatively(e.g. until there is sufficient empirical evidence showing the positive impact of the expertise).\n\n8.1.3: In relation to §161(1)(c) and §178(1)(e) of the BCR, AIs incorporating the risk mitigating effect of collateral in LGD estimation should recognize their potential inability to gain both control of the collateral and liquidate it expeditiously. AIs should establish a robust framework for managing collateral and ascertaining the legal certainty surrounding the control of collateral, with a set of comprehensive policies, operational procedures and risk management processes that are generally consistent with those required for the foundation IRB approach.\n\n8.1.4: In relation to §161(2)(c) and(d), and §178(2)(c) and(d) of the BCR, an AI should in its LGD estimation consider: (i)   the extent of dependence between the default risk of the obligor and the risks associated with the collateral or collateral provider.  AIs should address any material dependence in a conservative manner; and (ii)  any currency mismatch between the collateral and the underlying obligation and treat such mismatch conservatively.\n\n8.1.5: For certain types of facilities(or pools for retail exposures), loss severities may exhibit significant cyclical variability and LGD estimates may differ materially from the long run default-weighted average.  For these facility types and pools, AIs should incorporate the impact of economic downturn conditions into their LGD estimates as required by §161(1)(a) and §178(1)(a) of the BCR.\n\n8.1.6: In the process of identifying economic downturn conditions for estimation of LGD, AIs may make reference to the averages of loss severities observed during periods of high credit losses, forecasts(or extrapolation) based on appropriately conservative assumptions, or other similar methods(e.g. regression models).  Appropriate estimates of LGD during periods of high credit losses can be formed using internal data, external data or a combination of both.\n\n8.1.7: An AI should have a rigorous and well-documented process for assessing the effects of economic downturn conditions on recovery rates and for producing LGD estimates consistent with these conditions. The process must consist of the following components: (i)   the identification of appropriate downturn conditions for the AI’s exposures of each IRB class within each jurisdiction; (ii) the identification of adverse dependencies, if any, between default rates and recovery rates; and (iii) the incorporation of adverse dependencies, if identified, between default rates and recovery rates so as to produce LGD parameters for the AI’s exposures consistent with the identified downturn conditions.\n\n8.1.8: An AI should provide the HKMA with the long-run default-weighted average loss rate given default for every relevant facility type unless the AI can demonstrate to the HKMA that: (i)   its estimate of loss rate given default under downturn conditions is consistent with paragraphs8.1.6 and 8.1.7 above; and (ii)  it is not practical to report a separate estimate of long-run default-weighted average loss rate given default.\n\n### Section 8.2: Methods for assigning LGD to non-defaulted exposures\nMethods for assigning LGD to non-defaulted exposures\n\n8.2.1: The HKMA expects AIs to use one of the following methods to assign LGD to non-defaulted exposures: (i)  workout LGD which is based on observations of the discounted cash flows resulting from the workout process for defaulted facilities; (ii)  market LGD which is derived from observations of market prices on defaulted bonds or marketable loans soon after default; (iii) implied historical LGD which is inferred from an estimate of the expected long-run loss rate(which is based on the experience of total losses) of a portfolio(or a segment of a portfolio) and the PD estimate of that(segment of) portfolio.  This method is only allowed for deriving the LGD of retail exposures; and (iv) implied market LGD which is derived from non-defaulted risky bond prices through an asset-pricing model.\n\n8.2.2: For both the workout LGD and market LGD methods, AIs should be able to demonstrate to the HKMA that they have established appropriate methods to: (i)   determine which defaulted facilities are to be included in the development data set;\n\n8.2.3: For the implied historical LGD method for retail exposures, the validity of an LGD estimate will depend on that of the estimate of the expected long-run loss rate and that of the PD estimate.  Therefore, AIs should be able to demonstrate to the HKMA that the estimates of the expected long-run loss rate and the PD are appropriately determined.\n\n8.2.4: For the implied market LGD method, credit spreads of non-defaulted risky bonds are used.  The credit spreads, among other things, are decomposed into PD and LGD with an asset-pricing model.  The AIs should therefore be able to demonstrate to the HKMA the appropriateness of: (i)   the non-defaulted facilities that are included in the development data set; and (ii) the method used to decompose credit spreads into PD and LGD(i.e. the soundness of the asset-pricing model used).\n\n8.2.5: The HKMA expects AIs to be able to justify their choice of method for LGD estimation, and demonstrate a full understanding of the properties and limitations of the methods they use, and the applicability of these methods to different types of facilities(or pools for retail exposures).\n\n### Section 8.3: Assignment of LGD estimates to defaulted exposures\nAssignment of LGD estimates to defaulted exposures\n\n8.3.1: In relation to §161(1)(d) and §178(1)(f) of the BCR, the LGD assigned to a defaulted exposure should reflect the possibility that an AI would have to incur additional, unexpected losses during the recovery period.  The AI must also construct its best estimate of the expected loss on each defaulted exposure based on current economic circumstances and facility status as required by §220(2)(b) of the BCR.  The amount, if any, by which the LGD assigned to the defaulted exposure exceeds the AI’s best estimate of the expected loss on the exposure represents the capital requirement for that exposure according to§156(4) and §176(5) of the BCR as appropriate. Instances where the best estimate of the expected loss on a defaulted exposure is less than the sum of specific provisions and partial charge-offs for that exposure must be justified.  The details and the justification should be well documented for the HKMA’s scrutiny upon request.31\n\n### Section 8.4: LGD estimation process for workout and market LGD32 Construction of a development data set\nLGD estimation process for workout and market LGD32 Construction of a development data set\n\n8.4.1: The first step in LGD estimation is to construct a development data set containing the relevant information on defaulted facilities such as loss, recovery, risk factors etc.  An AI should be able to demonstrate to the HKMA that in constructing the development data set: (i)   there are no potential biases in selecting the defaulted facilities; (ii)  data for years with relatively frequent defaults and high realized LGD are included;\n\n8.4.2: After constructing the development data set, the realized LGD for each defaulted facility included in the development data set must be measured.  For workout LGD, this should involve all the aspects discussed in subsection 8.5.  For market LGD, an AI should be able to demonstrate that issues surrounding liquidity of the relevant markets for defaulted facilities and comparability of the instruments in the development data set to the AI’s portfolio have been adequately considered. Assignment of LGD estimates to non-defaulted facilities\n\n8.4.3: AIs should be able to demonstrate that they have conducted an analysis of the empirical distribution of realized LGD to detect problems related to outlying observations, changes in segmentation, and temporal homogeneity of the facilities included in the development data set.\n\n8.4.4: The HKMA expects AIs to evaluate the variability of realized LGD in the development data set as well as that of newly defaulted facilities for each facility type33.  The LGD assigned to the non-defaulted facilities should be adjusted upward if the variability is high, for instance, relative to the mean.\n\n8.4.5: AIs may use modelling techniques(e.g. a regression model) to directly derive, or to refine the LGD estimates. When models are used, the HKMA expects AIs to perform both out-of-time and out-of-sample tests in order to assess their true predictive power.\n\n8.4.6: Expert judgement should only be used to fine-tune the LGD estimates to the extent that the reasons for adjustments have not been taken into account in the estimation process.  The process of exercising expert judgement should be prudent, transparent, well-documented and closely monitored.\n\n8.4.7: AIs should compare the LGD estimates with the long-run default-weighted average loss rate given default for every relevant facility type to ensure that the former is not lower than the latter.\n\n### Section 8.5: Issues specific to workout LGD\nIssues specific to workout LGD\n\n8.5.1: Workout LGD is the most commonly used method in the industry.  The definition of when a workout ends, measurements of recoveries and costs, and the assumption on discount rates are crucial to computing the realized LGD for the defaulted facilities in the development data set. Definition of the end of a workout\n\n8.5.2: The HKMA expects AIs to define when a workout is finished using one of the following four options: (i)   a recovery threshold(e.g. when the remaining non-recovered value is less than 5% of the EAD); (ii)  a given time threshold(e.g. one year from the date of default); (iii) an event-based threshold(e.g. when repossession occurs); and (iv) a combination of(i),(ii) and/or(iii)(e.g. the earlier of one year from the date of default and when repossession occurs).\n\n8.5.3: Recoveries from a workout process can be cash recoveries and/or non-cash recoveries. (i)   Cash recoveries are relatively easy to measure and incorporate into the LGD calculations. (ii) Non-cash recoveries, especially those resulting from repossessions, are more difficult to track and are typically treated on a case-by-case basis for individual defaulted facilities in the development data set.\n\n8.5.4: There are two options for AIs to measure non-cash recoveries resulting from repossessions. (i)   The first option is to consider the recovery process complete at the time of the repossession. (ii) The second option is to consider the recovery process complete only when the repossessed asset has been sold to a third party.\n\n8.5.5: If AIs choose to adopt the first option, they should apply a haircut coefficient to the book value of the repossessed asset to convert the associated non-cash recovery into an artificial cash recovery.  AIs should calibrate the haircut coefficient based on historical experience(e.g. historical volatility of asset value and time required for selling the asset to a third party), taking into account the impact of economic downturn conditions as appropriate.\n\n8.5.6: AIs must include all the costs, including both direct costs and indirect costs, of the workout process in the calculation of LGD, taking account of the possibility that AIs will have to incur unexpected costs during the debt recovery period. (i)   Direct costs are those associated with a particular facility(e.g. a fee for an appraisal of collateral). (ii)  Indirect costs are those necessary to carry out the recovery process but not associated with individual facilities(e.g. overheads associated with the office space for the workout department).\n\n8.5.7: The HKMA generally expects AIs to identify the key recovery costs for each product, to model them using a sample of defaulted facilities for which the true costs(both direct and indirect costs) are known, and to allocate the costs of recoveries out of the sample using the model. Choice of discount rate\n\n8.5.8: To calculate the economic loss of a defaulted facility, it is necessary to discount the observed recoveries and costs back to the date of default using some discount rates. The HKMA recognizes two options that can be used by AIs: historical discount rates and current discount rates. (i)   Historical discount rates are fixed for each defaulted facility, regardless of the date on which the LGD is being estimated.  All of the cash flows associated with a defaulted facility are discounted using a rate determined at a particular date in the life of the defaulted facility.  Alternatively, at the date of default a discount rate curve can be constructed with rates for each date over the expected life of the workout and the cash flows can be discounted using the curve.  Typically, the discount rate is defined as either the risk-free rate plus a spread at the default date for the average recovery period, a suitable rate for an asset of similar risk at the default date, or a zero-coupon yield plus a spread at the default date.\n\n8.5.9: The HKMA expects AIs to use either method of calculating discount  rates   in a consistent and conservative manner.  The guiding principle is that the selected discount rates should reflect the cost of holding defaulted assets over the workout period and include an appropriate risk premium which is commensurate with the risks of the recovery.  Specifically, the higher the uncertainty about the recovery in respect of a defaulted facility, the higher the discount rate that will be expected.\n\n8.5.10: The discount rate applied should reflect the underlying risk of the transaction and the type and nature of the security available to the AI.  A risk-free rate should only be used when the recovery is: (i)   expected to come from liquidation of cash collateral with certainty; or (ii)  converted to a certainty-equivalent cash flow34.\n\n8.5.11: In cases where the recovery is expected to arise from entering into a new contract to pay(e.g. restructuring) or\n\n8.5.12: When the recovery is expected to come from a third party(e.g. a guarantor), the discount rate should reflect the risk associated with that third party.\n\n8.5.13: The HKMA does not generally expect AIs to use the cost of capital, the cost of funding or the cost of equity as the discount rates, as these rates do not reflect the risk of recovery of a defaulted facility.  The HKMA generally expects that the discount rate used by an AI will vary by type of product/facility in order to reflect the differences in the risk of recovery.  However, the HKMA may consider permitting an AI to use the same discount rate across different products/facilities, provided that it is able to demonstrate to the HKMA that: (i)   such rate is sufficiently conservative as regards the products/facilities to which the rate is applied; or (ii) the products/facilities share a similar level of risk in their recoveries.\n\n### Section 8.6: Validation of LGD estimates\nValidation of LGD estimates\n\n8.6.1: AIs should be able to demonstrate that they have performed the following analyses and tests on their estimates of LGD: (i)   Stability analysis: To assess if LGD estimates are stable and robust, AIs should analyse: ● how changes in the development data set(e.g. use of sub-samples) and changes in the assumptions made for determining the realized LGD and/or parameters of the model impact the LGD estimates; and\n\n### Section Annex B: Section Annex B\nStatistical methodologies in validating calibration\n\n### Section Annex B: Section Annex B\nStatistical methodologies in validating calibration51 B1.   Binomial test with assumption of independent default events B1.1 Consider a rating model assigning obligors to a set of k obligor grades(or pools for retail exposures) K={K1, K2,..., Kk}.  For obligor grade Ki, assume that there are NDi defaulters and NNDi non-defaulters.  For each obligor grade(or pool for retail exposures, but not score), the binomial test with assumption of zero default correlation can be conducted based on the following hypotheses: Null hypothesis(H0):         The PD of an obligor grade is correct. Alternative hypothesis(H1):   The PD of an obligor grade is underestimated. B1.2 Given a confidence level q(e.g. 99%), the null hypothesis is rejected if the number of observed defaults NDi in obligor grade Ki is greater than a critical value NDi*, which is defined as: min Ng2'=mioNs12[][(5-) 0. wherePDiis the forecast of default probability for the obligor grade and Ni is the number of obligors assigned to the obligor grade(i.e. NDi+ NNDi). The critical value NDi* can be approximated by: No~01(q),NPD[1-PD) PD     PD +NPDi, where where p-7 denotes the inverse cumulative distribution function of the standard normal distribution.  The critical value can be expressed in terms of an observed default rate PDi* that is allowed at maximum:",
  "13": "### Section 9.1: Supervisory expectations for estimation of EAD Credit management\nSupervisory expectations for estimation of EAD Credit management\n\n9.1.1: EAD can be sensitive to the way that AIs manage credits and changes therein.  The HKMA expects AIs to: (i)   have a process in place for ensuring that estimates of EAD take into account these practices and relevant changes.  In particular, an AI should raise its EAD estimates immediately if such changes are expected to cause credit conversion factors(CCFs) of the relevant facility types to increase materially. However, downward adjustments to CCFs that may potentially result from such changes should be made only after a significant amount of actual experience has been accumulated to justify such adjustments;\n\n9.1.2: For the purposes of §164(4)(c) and(ca) of the BCR, the supervisory expectations set out in paragraphs 8.1.6 to8.1.8 are also applicable to EAD estimation in general. For AIs which are able to develop their own EAD models, this could be achieved by, for example, considering the cyclical nature, if any, of the drivers of such models.  AIs which do not have sufficient data but have to rely on external data in their assessment of the impact of economic downturn should make use of such data conservatively.\n\n### Section 9.2: EAD estimation process for non-defaulted facilities\nEAD estimation process for non-defaulted facilities\n\n9.2.1: The estimation process of EAD for non-defaulted facilities in general involves the following steps: (i)  A development data set38storing information on defaulted facilities(including the relevant risk factors) is first constructed(see paragraphs 9.2.4 to 9.2.7).\n\n9.2.2: An AI’s EAD estimates should be based on data that reflect the obligor, facility and its management practice characteristics of the exposures to which the estimates are applied.  Consistent with this principle, EAD estimates should be based on appropriately homogenous segments, or based on an estimation approach that effectively disentangles the impact of the different characteristics exhibited within the development data set.  On the other hand, EAD estimates applied to particular exposures should not be based on data that comingle the effects of disparate characteristics or data from exposures that exhibit different characteristics, such as: (i)   same broad product grouping but different customers that are managed differently by the AI(e.g. SME/middle market data being applied to large corporate obligors); (ii)  data from commitments with small unused credit limits being applied to facilities with large unused limits; (iii) data from obligors already identified as problematic at observation point(e.g. obligors who were already delinquent, put on the AI’s watch list, blocked from further drawdowns, subject to recent limit reduction initiated by the AI or other types of collection activities etc. at 12 months before such obligors defaulted) being applied to current obligors with no known issues; and\n\n9.2.3: The expectations set out in paragraphs 8.4.3 to 8.4.7 on LGD estimation are also applicable to the estimation of CCF for non-defaulted facilities. Construction of development data set\n\n9.2.4: AIs should use the 12-month fixed-horizon approach to construct their development data sets for EAD estimation, i.e. for each observation in the development data set, default outcomes must be linked to relevant obligor and facility characteristics twelve months prior to default.  As an example, with a 12-month fixed interval, if a default event occurred on 15 July 2024, then in addition to the outstanding amount upon default, information about risk factors of the defaulted facility 12 months ago(the observation point is then 15 July 2023) is to be used.\n\n9.2.5: EAD data must not be capped to the principal amount outstanding or facility limits.  Accrued interest, other due payments and limit excesses should be included in the EAD data.\n\n9.2.6: Data of facilities that have defaulted, but have subsequently been recovered, should also be included.\n\n9.2.7: For EAD data which have been affected by product profile transformation over the observation period, an AI should be able to demonstrate to the HKMA that it has a detailed understanding of the impact of the transformation on their CCF/EAD estimates, and that the impact is immaterial or has been effectively mitigated within the AI’s EAD estimation process.  The HKMA in general does not consider the following as effective mitigating measures:\n\n9.2.8: In relation to §164(4)(e)(ii) and §180(1) of the BCR, the criteria or risk factors considered by an AI in deriving its CCF/EAD estimates must be plausible and intuitive, and represent what the AI believes to be the material drivers of EAD.  The HKMA expects the AI to demonstrate that its choices are supported by credible internal analysis, and be able to provide a breakdown of its EAD experience by the factors it sees as the drivers of EAD.  The AI should use all relevant and material information in deriving its CCF/EAD estimates, and review these estimates across facility types when material new information becomes available and at least on an annual basis.\n\n9.2.9: AIs are recommended to take into account the following types of factors in their EAD estimation process: (i)  factors affecting the obligor’s demand for funding/facilities; (ii) factors affecting the AI’s willingness to supply funding/facilities; (iii) the attitude of third parties(e.g. other AIs, money lenders, trade creditors and owners if the obligor is a company) who can act as alternative sources of funding supply available to the obligor; and\n\n9.2.10: An AI may use obligors’ outstanding balances(including accrued but unpaid interest and fees) at the reporting dates of capital adequacy ratios as its EAD estimates for facility types which only involve on-balance sheet exposures(e.g. term loans).  For facilities with off-balance sheet exposures(e.g. credit lines, commitments and guarantees), an AI may take 100% of credit limits40 at the reporting date as the EAD estimates.  An AI using either or both of these methods should demonstrate that the estimated aggregate EAD amount for each of the facility types is higher than the realized aggregate EAD amount for that facility type(see paragraph 9.3.2).  An AI using100% of credit limits as the EAD estimates for facilities with off-balance sheet exposures should develop a plan agreeable to the HKMA to estimate the CCFs for such facilities.\n\n9.2.11: The undrawn limit factor(ULF) approach41 is a commonly used method in estimating CCF for facilities with off-\n\n### Section 9.3: Validation of EAD estimates\nValidation of EAD estimates\n\n9.3.1: AIs should be able to demonstrate that they have conducted the same types of analyses and tests used for assessing LGD estimates(see paragraph 8.6.1) in their assessment of the accuracy of EAD estimates in terms of CCF.  The expectations set out in paragraph 3.5.7 are also applicable to the validation of CCF.\n\n9.3.2: Where AIs use obligors’ outstanding balances as EAD estimates for facilities which only involve on-balance sheet exposures, or use obligors’ credit limits as EAD estimates for facilities which involve off-balance sheet exposures(see paragraph 9.2.10), the HKMA does not normally expect them to conduct the analyses and assessments described in paragraph 9.3.1 for validating the accuracy of the relevant EAD estimates.  However, AIs should be able to demonstrate no less than once every 12 months that these EAD estimates are sufficiently conservative43.  In particular, the HKMA expects AIs to:\n\n### Section Annex C: Section Annex C\nPossible risk factors in estimation of EAD\n\n### Section Annex C: Section Annex C\nPossible risk factors in estimation of EAD C1.   Type of obligor C1.1 The differentiation of obligor types is relevant with regard to varying behaviour in credit line utilization.  For example, for large-scale obligors(such as large corporates and banks), lines of credit are often not completely utilized at the time of default.  In contrast, retail customers and SMEs are more likely to overdraw(or fully utilize) the approved lines of credit. C2.   Relationship between an AI and obligor in adverse circumstances C2.1 EAD often depends on how the relationship between an AI and obligor evolves in adverse circumstances, when the obligor may decide to draw unused commitments. C3.   Alternative sources of funds available to the obligor C3.1 The more the obligor has access to alternative sources and forms of credit, the lower the EAD is expected to be.  For example, retail customers and SMEs in general have less access to alternative sources than large corporate obligors and banks.  In cases where this factor cannot be observed, AIs may apply the “type of obligor” factor as a proxy for it. C4.   Covenants C4.1 Some empirical findings indicate that the draw-down of a credit line at the time of default tends to decrease with the quality of the obligor’s credit rating at the time the commitment was granted.  This observation may be due to the fact that a bank is more likely to require covenants for obligors with lower credit quality which restrict future draw-downs in cases where the credit quality has declined.",
  "14": "### Section 10.1: Types of LDPs\nTypes of LDPs\n\n10.1.1: A key characteristic of LDPs is that AIs lack sufficient default and loss data in respect of these portfolios.  This presents challenges for risk quantification and validation.\n\n### Section 10.2: Implications for risk quantification and validation\nImplications for risk quantification and validation\n\n10.2.1: An AI should consider whether any of its portfolios have the characteristics of an LDP.  If so, the AI should design specific risk quantification and validation methodologies appropriate for such portfolios, as each type of LDPs has quite  different   risk   characteristics   with  varying implications for risk quantification and validation.  In particular, AIs should be able to demonstrate that they have taken into account the considerations in paragraphs10.2.3 to 10.2.6, which extend the Basel IRB validation principles.\n\n10.2.2: AIs should note that the techniques outlined in paragraphs 10.2.3 to 10.2.6 are tools to enhance the reliability of the credit risk component estimates for LDPs. The applicability of a particular technique is likely to vary between AIs.  AIs may also use techniques other than those described in this module.  In all cases, AIs will need to justify their chosen techniques, document the limitations and apply conservatism to the results where necessary.\n\n10.2.3: Credit risk component estimates are intended to be forward-looking.  Therefore, the relative scarcity of historical default and loss data in some circumstances may not be a serious impediment to developing PD and, where applicable, LGD and EAD estimates.  Where, for example, there is a lack of recent loss data, but other analysis suggests that the potential risk of loss in a portfolio is not negligible(type(iv) in paragraph 10.1.1), AIs may base the credit risk component estimates not solely on recent loss data, but also on additional information about the drivers of default and losses.  For example, AIs can use default and loss experience of similar asset classes in other geographical locations in risk quantification or validation.  Taking a longer run of data would be another option provided that the data are available. Data-enhancing techniques\n\n10.2.4: Where the problem of limited loss data exists at the level of an individual AI, the HKMA expects the AI to make use of techniques such as pooling of data with other financial institutions or market participants, acquire and utilize data from other external sources, or apply market measures of risk, to compensate for its lack of internal loss data.  An AI would need to satisfy itself and the HKMA that the external or pooled data are relevant to its own situation(see subsection 6.9).  This technique is especially relevant to small portfolios(type(ii) in paragraph 10.1.1) and to portfolios where an AI is a recent market entrant(type(iii) in paragraph 10.1.1).\n\n10.2.5: For some portfolios, such as type(i) in paragraph 10.1.1above, there may be limited loss data not just at the level of an individual AI, but also across the industry.  In these cases, the HKMA expects AIs to demonstrate the use of some or all of the following techniques to enhance data richness45:\n\n10.2.6: When AIs do not have sufficient default and loss data(even if data-enhancing techniques are used) to back-test the accuracy of their rating systems including the associated credit risk component estimates, the HKMA expects them to use benchmarking tools to demonstrate that their rating systems and the credit risk component estimates are accurate.  Section 11 gives details on the use of benchmarking tools in validation.",
  "15": "### Section 2.1: The HKMA’s approach to validation adheres to the principles promulgated by the Basel Committee 2(“Basel IRB validation principles”) as follows: (i)    Validation is fundamentally about assessing the predictive ability of a bank’s risk estimates and the use of ratings in credit processes; (ii)   The bank has primary responsibility for validation; (iii)   Validation is an iterative process; (iv)   There is no single validation method; (v)   Validation should encompass both quantitative and qualitative elements; and (vi)   Validation processes and outcomes should be subject to independent review.\nThe HKMA’s approach to validation adheres to the principles promulgated by the Basel Committee 2(“Basel IRB validation principles”) as follows: (i)    Validation is fundamentally about assessing the predictive ability of a bank’s risk estimates and the use of ratings in credit processes; (ii)   The bank has primary responsibility for validation; (iii)   Validation is an iterative process; (iv)   There is no single validation method; (v)   Validation should encompass both quantitative and qualitative elements; and (vi)   Validation processes and outcomes should be subject to independent review.\n\n### Section 2.2: Consistent with the second principle, it is an AI’s responsibility to demonstrate to the satisfaction of the MA that its rating systems and the relevant processes meet the applicable HKMA requirements, including that the AI has a reliable system for validating regularly the accuracy and consistency of its rating systems as required by §1(i) of Schedule 2 to the BCR.  This, together with other Basel IRB validation principles, forms the basis of the HKMA’s approach to validation that the AI is required to conduct its own validation, document clearly the validation processes and results, and share them with the HKMA for review in the IRB recognition process and ongoing supervision.\nConsistent with the second principle, it is an AI’s responsibility to demonstrate to the satisfaction of the MA that its rating systems and the relevant processes meet the applicable HKMA requirements, including that the AI has a reliable system for validating regularly the accuracy and consistency of its rating systems as required by §1(i) of Schedule 2 to the BCR.  This, together with other Basel IRB validation principles, forms the basis of the HKMA’s approach to validation that the AI is required to conduct its own validation, document clearly the validation processes and results, and share them with the HKMA for review in the IRB recognition process and ongoing supervision.\n\n### Section 2.3: The HKMA’s approach to validation consists of two key components.  Focusing on the qualitative aspects, the first component involves a review of the AI’s processes, procedures and controls that are in place for rating systems.  This includes, for example, an assessment of whether these systems are subject to adequate oversight by the AI’s Board of Directors3 and senior management, both before and during use; whether adequate procedures are in place to ensure the integrity and reliability of the data used; and whether the rating systems are validated at an appropriate frequency by individuals who have relevant knowledge and experience to do so and are independent of the parties that have been involved in developing the rating systems.  Internal and external auditors of the AI should also be involved in the processes.  The expectations of the HKMA in these areas are set out in sections 4 to 6.\nThe HKMA’s approach to validation consists of two key components.  Focusing on the qualitative aspects, the first component involves a review of the AI’s processes, procedures and controls that are in place for rating systems.  This includes, for example, an assessment of whether these systems are subject to adequate oversight by the AI’s Board of Directors3 and senior management, both before and during use; whether adequate procedures are in place to ensure the integrity and reliability of the data used; and whether the rating systems are validated at an appropriate frequency by individuals who have relevant knowledge and experience to do so and are independent of the parties that have been involved in developing the rating systems.  Internal and external auditors of the AI should also be involved in the processes.  The expectations of the HKMA in these areas are set out in sections 4 to 6.\n\n### Section 2.4: The second component of the HKMA’s approach to validation focuses on the regular use of at least some of the generally accepted quantitative techniques by the AI in assessing the performance of its rating systems and accuracy of the credit risk component estimates.  The quantitative techniques presented in sections 7 to 11 reflect some common market practices in the estimation and validation of rating systems and the credit risk components.\nThe second component of the HKMA’s approach to validation focuses on the regular use of at least some of the generally accepted quantitative techniques by the AI in assessing the performance of its rating systems and accuracy of the credit risk component estimates.  The quantitative techniques presented in sections 7 to 11 reflect some common market practices in the estimation and validation of rating systems and the credit risk components.\n\n### Section 2.5: In line with the fourth principle, the HKMA recognizes that there is no universal tool that can be used for the validation of all rating systems.  It therefore expects the design of a validation methodology to depend on the type of rating system and the underlying portfolio.  For example, back-testing may be useful for validating the credit risk component estimates for retail portfolios in general.  However, it may be less applicable to portfolios with a small number of historical defaults where benchmarking may be a more useful validation tool.\nIn line with the fourth principle, the HKMA recognizes that there is no universal tool that can be used for the validation of all rating systems.  It therefore expects the design of a validation methodology to depend on the type of rating system and the underlying portfolio.  For example, back-testing may be useful for validating the credit risk component estimates for retail portfolios in general.  However, it may be less applicable to portfolios with a small number of historical defaults where benchmarking may be a more useful validation tool.\n\n### Section 2.6: The HKMA also notes that the techniques, especially the quantitative techniques, for validation of rating systems and the credit risk component estimates are very diverse, portfolio-specific and evolving.  Therefore, the HKMA neither prescribes specific techniques nor sets precise quantitative minimum standards that should be employed for validation.  AIs are expected to apply the validation techniques and practices 4(including the parameters adopted for validation) that are commonly used in the industry for specific types of rating systems and portfolios.  When an AI employs a validation technique which differs from that in widespread use by its peers, the HKMA expects it to be able to justify its choice.  Where appropriate, the HKMA may require the AI to apply a specific validation technique to a rating system/portfolio and to submit the validation results for review.\nThe HKMA also notes that the techniques, especially the quantitative techniques, for validation of rating systems and the credit risk component estimates are very diverse, portfolio-specific and evolving.  Therefore, the HKMA neither prescribes specific techniques nor sets precise quantitative minimum standards that should be employed for validation.  AIs are expected to apply the validation techniques and practices 4(including the parameters adopted for validation) that are commonly used in the industry for specific types of rating systems and portfolios.  When an AI employs a validation technique which differs from that in widespread use by its peers, the HKMA expects it to be able to justify its choice.  Where appropriate, the HKMA may require the AI to apply a specific validation technique to a rating system/portfolio and to submit the validation results for review.\n\n### Section 2.7: The HKMA may require an AI to provide its credit risk component estimates and the relevant data for comparison with other AIs’other estimates for similar obligors/facilities in order to identify potential outlying predictions.\nThe HKMA may require an AI to provide its credit risk component estimates and the relevant data for comparison with other AIs’other estimates for similar obligors/facilities in order to identify potential outlying predictions.\n\n### Section 2.8: The HKMA may request an AI to use an alternative approach to estimate the credit risk components(e.g. a different segmentation approach for retail exposures) and compare the results against the estimates generated by the method adopted or proposed by the AI.\nThe HKMA may request an AI to use an alternative approach to estimate the credit risk components(e.g. a different segmentation approach for retail exposures) and compare the results against the estimates generated by the method adopted or proposed by the AI.\n\n### Section 2.9: Where the HKMA considers appropriate, it may require an AI to commission a report from its external auditors or other independent experts with the relevant expertise, experience and track record in such work to review the AI’s compliance with the applicable HKMA requirements.\nWhere the HKMA considers appropriate, it may require an AI to commission a report from its external auditors or other independent experts with the relevant expertise, experience and track record in such work to review the AI’s compliance with the applicable HKMA requirements.\n\n### Section 3.5: Accuracy of rating systems\nAccuracy of rating systems\n\n3.5.1: Another important factor affecting an AI’s eligibility for using the IRB approach is whether the AI has a robust system in place to validate the accuracy and consistency of its rating systems, processes, and the associated credit risk component estimates, and whether the validation process enables the AI to assess the performance of its rating systems consistently and meaningfully.\n\n3.5.2: Specifically, AIs should regularly compare realized default rates with their estimates of PD for each grade(or each pool for retail exposures) and be able to demonstrate that the realized default rates are within the expected range for that grade(or pool).  AIs using the advanced and retail IRB approaches should perform such analysis for their estimates of LGD and EAD.  Such comparisons should make use of relevant and material historical data that are over as long a period as possible.  AIs should clearly document the methods and data used in such comparisons, and update the analysis and documentation at least annually.\n\n3.5.3: AIs should also use other quantitative validation tools and comparisons with relevant external data sources(see section 11 on benchmarking). The analysis should be based on data that are appropriate to the portfolio, are updated regularly, and cover a relevant observation period.  AIs’ assessments of the performance of their rating systems should be based on long data histories, covering a range of economic conditions, and ideally one or more complete economic cycles.\n\n3.5.4: AIs may use the quantitative validation methods cited in this module or other methods for performing the above analyses.  For the latter, the AIs should be able to demonstrate to the HKMA that the techniques are theoretically  sound,   well-documented,  consistently applied and able to meet the standards applicable to the generally accepted quantitative techniques.  The AIs should be able to provide the rationale for choosing the techniques and demonstrate the appropriateness of using such techniques.\n\n3.5.5: AIs should demonstrate that their quantitative validation methods and other validation methods do not vary systematically with the economic cycle.  Changes in methods and data(both data sources and periods covered) should be justified and clearly documented.\n\n3.5.6: AIs should have well-articulated internal standards for situations where deviations in realized values of the credit risk components from expectations become significant enough to call the validity of the estimates into question. These standards should take account of economic cycles and similar systematic variability in the AIs’ default and loss experiences.  AIs should put in place a framework for revising the credit risk component estimates upward to reflect their default and loss experiences when realized values continue to be higher than the expected values.\n\n3.5.7: In practice, the HKMA expects AIs to establish tolerance limits for the differences between credit risk component estimates and the realized values.  AIs should have a clearly documented policy that requires remedial actions to be taken when the tolerance limits are exceeded.  The internal tolerance limits and remedial actions should be commensurate with the risk that the computed capital requirement would not be adequate to cover the default risk and credit loss incurred.  In setting its internal standards and determining any remedial actions to a breach of those standards, an AI should be able to demonstrate that it has taken into account a range of factors, including but not limited to the relative sizes of the portfolios to which the rating systems are applied, the AI’s risk appetite in respect of the portfolios, the distribution of the portfolios amongst rating grades(or pools for retail exposures), and the inherent risk characteristics of the portfolios.\n\n3.5.8: In general, estimates of the credit risk components are likely to involve unpredictable errors.  In order to avoid over-optimism, AIs should add to their estimates a margin of conservatism that is related to the likely range of errors. Where performance of a rating system, and the methods and data used are less satisfactory and the likely range of errors is larger, the margin of conservatism should be larger9.\n\n3.5.9: An AI must also have a set of procedures to evaluate the appropriateness of the method or data used in estimation of the credit risk components, and there is a mechanism for adjusting the estimates in response to uncertainty stemming from the data, processes or methodologies used by the AI(e.g. by adding a margin of conservatism for any likely range of errors).\n\n3.5.10: Where AIs rely on supervisory estimates of LGD and EAD, rather than their own internal estimates, they are encouraged to compare the realized values of LGD and EAD to the supervisory estimates. The information on the realized values of LGD and EAD should form part of the AIs’ assessment of economic capital.",
  "16": "### Section 3.5.5: 3.5 Subsection\nAIs should demonstrate that their quantitative validation methods and other validation methods do not vary systematically with the economic cycle.  Changes in methods and data(both data sources and periods covered) should be justified and clearly documented.\n\n### Section 3.5.6: 3.5 Subsection\nAIs should have well-articulated internal standards for situations where deviations in realized values of the credit risk components from expectations become significant enough to call the validity of the estimates into question. These standards should take account of economic cycles and similar systematic variability in the AIs’ default and loss experiences.  AIs should put in place a framework for revising the credit risk component estimates upward to reflect their default and loss experiences when realized values continue to be higher than the expected values.\n\n### Section 7.3.2: 7.3 Subsection\nThe HKMA expects AIs to demonstrate that their rating systems exhibit stable discriminatory power.  Therefore, in addition to in-sample validation, AIs should be able to demonstrate their rating systems’ discriminatory power on an out-of-sample and out-of-time basis.  This is to ensure that the discriminatory power is stable on data sets that are cross-sectionally or temporally independent of,but structurally similar26 to, the development data set.  If out-of-sample and out-of-time validations cannot be conducted due to data constraints, AIs are expected to employ statistical techniques such as k-fold cross validation27 or bootstrapping28 for this purpose.  When an AI uses these statistical techniques, it should be able to provide the rationale for using these techniques and demonstrate the appropriateness of the chosen techniques, and understand the limitations, if any, of these techniques.",
  "17": "### Section 2.8: The HKMA may request an AI to use an alternative approach to estimate the credit risk components(e.g. a different segmentation approach for retail exposures) and compare the results against the estimates generated by the method adopted or proposed by the AI.\nThe HKMA may request an AI to use an alternative approach to estimate the credit risk components(e.g. a different segmentation approach for retail exposures) and compare the results against the estimates generated by the method adopted or proposed by the AI.\n\n### Section 2.9: Where the HKMA considers appropriate, it may require an AI to commission a report from its external auditors or other independent experts with the relevant expertise, experience and track record in such work to review the AI’s compliance with the applicable HKMA requirements.\nWhere the HKMA considers appropriate, it may require an AI to commission a report from its external auditors or other independent experts with the relevant expertise, experience and track record in such work to review the AI’s compliance with the applicable HKMA requirements.\n\n### Section 3.5.7: 3.5 Subsection\nIn practice, the HKMA expects AIs to establish tolerance limits for the differences between credit risk component estimates and the realized values.  AIs should have a clearly documented policy that requires remedial actions to be taken when the tolerance limits are exceeded.  The internal tolerance limits and remedial actions should be commensurate with the risk that the computed capital requirement would not be adequate to cover the default risk and credit loss incurred.  In setting its internal standards and determining any remedial actions to a breach of those standards, an AI should be able to demonstrate that it has taken into account a range of factors, including but not limited to the relative sizes of the portfolios to which the rating systems are applied, the AI’s risk appetite in respect of the portfolios, the distribution of the portfolios amongst rating grades(or pools for retail exposures), and the inherent risk characteristics of the portfolios.",
  "18": "### Section 4.1: Effective oversight by an AI’s Board of Directors and senior management is critical to a sound rating system.  In addition to the provisions set out in this module, AIs should also refer to CG-1\"Corporate Governance of Locally Incorporated Authorized Institutions\" and IC-1\"Risk Management Framework\" for details of their risk management responsibilities.  Many of the provisions and practices therein have a general application which is relevant to the use of the IRB approach.\nEffective oversight by an AI’s Board of Directors and senior management is critical to a sound rating system.  In addition to the provisions set out in this module, AIs should also refer to CG-1\"Corporate Governance of Locally Incorporated Authorized Institutions\" and IC-1\"Risk Management Framework\" for details of their risk management responsibilities.  Many of the provisions and practices therein have a general application which is relevant to the use of the IRB approach.\n\n### Section 4.10: For AIs extending the use of rating systems to their operations and credit risk exposures outside Hong Kong for regulatory capital reporting to the HKMA, they should exercise effective oversight and governance of the relevant subsidiaries/branches to ensure these entities’ compliance with the applicable HKMA requirements and the AIs’ established policies for implementing the IRB approach.\nFor AIs extending the use of rating systems to their operations and credit risk exposures outside Hong Kong for regulatory capital reporting to the HKMA, they should exercise effective oversight and governance of the relevant subsidiaries/branches to ensure these entities’ compliance with the applicable HKMA requirements and the AIs’ established policies for implementing the IRB approach.\n\n### Section 4.11: The HKMA will look for evidence of the Board and senior management involvement in IRB implementation, and their understanding of the rating systems during both the initial IRB recognition process and the ongoing review of such systems to ensure continuous compliance with the applicable HKMA requirements.\nThe HKMA will look for evidence of the Board and senior management involvement in IRB implementation, and their understanding of the rating systems during both the initial IRB recognition process and the ongoing review of such systems to ensure continuous compliance with the applicable HKMA requirements.\n\n### Section 4.2: The HKMA expects the Board and senior management of an AI to be actively involved in the implementation of the IRB approach at inception and on an ongoing basis, although the degree of attention and the level of detail that the Board and senior management need to comprehend will vary depending on their particular oversight responsibilities.  At a minimum, the Board and senior management of an AI must approve all the key elements of, and any material changes to, the AI’s rating systems; possess an adequate understanding of the design and operations of, and the management reports generated on aspects related to the AI’s rating systems; and exercise oversight sufficient to ensure the AI’s compliance with the applicable HKMA requirements.  The approval for the key elements of a rating system to be adopted by the AI should normally rest with the Board, or the regional or head office in the case of local subsidiaries.\nThe HKMA expects the Board and senior management of an AI to be actively involved in the implementation of the IRB approach at inception and on an ongoing basis, although the degree of attention and the level of detail that the Board and senior management need to comprehend will vary depending on their particular oversight responsibilities.  At a minimum, the Board and senior management of an AI must approve all the key elements of, and any material changes to, the AI’s rating systems; possess an adequate understanding of the design and operations of, and the management reports generated on aspects related to the AI’s rating systems; and exercise oversight sufficient to ensure the AI’s compliance with the applicable HKMA requirements.  The approval for the key elements of a rating system to be adopted by the AI should normally rest with the Board, or the regional or head office in the case of local subsidiaries.\n\n### Section 4.3: For the initial adoption of the IRB approach or any subsequent significant overhauls of the constituent rating systems, the Board of an AI may delegate an appropriate party(e.g. a project steering committee    广*or  implementation  team  comprising  senior management from the relevant business, credit, finance, IT, operations, and other support or control functions) to oversee and ensure the proper implementation of the IRB approach or any significant changes to it according to a pre-defined plan.  Such delegation may come directly from the regional or head office for local subsidiaries.\nFor the initial adoption of the IRB approach or any subsequent significant overhauls of the constituent rating systems, the Board of an AI may delegate an appropriate party(e.g. a project steering committee    广*or  implementation  team  comprising  senior management from the relevant business, credit, finance, IT, operations, and other support or control functions) to oversee and ensure the proper implementation of the IRB approach or any significant changes to it according to a pre-defined plan.  Such delegation may come directly from the regional or head office for local subsidiaries.\n\n### Section 4.4: The Board should ensure that sufficient resources are provided for implementing the project and that it is regularly kept informed of the progress in implementation and any slippages.  Where the AI is a local subsidiary, efforts must be made locally to meet this requirement10.  Where slippages in the project implementation plan are likely to have a significant effect on the AI’s ability to comply with the applicable HKMA requirements, the Board and the HKMA should be informed as soon as possible.\nThe Board should ensure that sufficient resources are provided for implementing the project and that it is regularly kept informed of the progress in implementation and any slippages.  Where the AI is a local subsidiary, efforts must be made locally to meet this requirement10.  Where slippages in the project implementation plan are likely to have a significant effect on the AI’s ability to comply with the applicable HKMA requirements, the Board and the HKMA should be informed as soon as possible.\n\n### Section 4.5: AIs are expected to conduct a comprehensive and independent validation of their rating systems at least annually, or when there are material changes in the market environment or business activities of the institutions that might have a significant impact on the use of the rating systems.  Nonetheless, it will be acceptable for an AI to conduct the validation exercise on a rolling basis, provided that the arrangements are justified by valid operational considerations, approved by the senior management, and the validation cycle for each portfolio(or component of a rating system, depending on the AI’s design of its validation programme) is initiated no more than 12 months and finished within 18 months after the completion of the previous cycle.  An AI should be able to demonstrate to the HKMA that the performance of its rating systems is robust and stable over time.  Regardless of how an AI implements its validation programme to meet this annual requirement, reports containing adequate information on the validation results should be reviewed and subject to deliberation by the Board.\nAIs are expected to conduct a comprehensive and independent validation of their rating systems at least annually, or when there are material changes in the market environment or business activities of the institutions that might have a significant impact on the use of the rating systems.  Nonetheless, it will be acceptable for an AI to conduct the validation exercise on a rolling basis, provided that the arrangements are justified by valid operational considerations, approved by the senior management, and the validation cycle for each portfolio(or component of a rating system, depending on the AI’s design of its validation programme) is initiated no more than 12 months and finished within 18 months after the completion of the previous cycle.  An AI should be able to demonstrate to the HKMA that the performance of its rating systems is robust and stable over time.  Regardless of how an AI implements its validation programme to meet this annual requirement, reports containing adequate information on the validation results should be reviewed and subject to deliberation by the Board.\n\n### Section 4.6: Senior management are responsible for the day-to-day operations of an AI, and should have a good general understanding of the AI’s rating systems.  Except in the case of local subsidiaries, which may need to adopt group-developed rating systems, senior management should take a leading role in determining the rating systems that the AI plans to adopt based upon the technical support of internal staff and/or external parties with the relevant expertise.\nSenior management are responsible for the day-to-day operations of an AI, and should have a good general understanding of the AI’s rating systems.  Except in the case of local subsidiaries, which may need to adopt group-developed rating systems, senior management should take a leading role in determining the rating systems that the AI plans to adopt based upon the technical support of internal staff and/or external parties with the relevant expertise.\n\n### Section 4.7: To ensure that the rating systems work consistently and as intended on an ongoing basis, senior management of an AI should: (i)   allocate and maintain sufficient resources(including IT) and internal staff expertise for the development, implementation, support, review and validation of the rating systems to ensure continuing compliance with the applicable HKMA requirements; (ii)  clearly delineate and assign the responsibilities and accountabilities   for   the   effective   operations   and maintenance of the rating systems to the respective business, credit, finance, IT, operations and other support or control functions, or personnel; (iii) ensure that adequate training on the rating systems is provided for staff in the relevant business, credit, finance, IT, operations and other support or control functions; (iv) make necessary changes to the existing policies and procedures as well as systems and controls in order to integrate the use of the rating systems into the AI’s credit risk management processes and culture; (v) ensure that the rating systems are put to use properly; (vi) ensure that the usage of the rating systems extends beyond purely regulatory capital reporting to decision-making and monitoring processes including credit approval, limits setting, credit monitoring and reporting, pricing, internal capital allocation, provisioning etc.(see  subsection 5.4);\nTo ensure that the rating systems work consistently and as intended on an ongoing basis, senior management of an AI should: (i)   allocate and maintain sufficient resources(including IT) and internal staff expertise for the development, implementation, support, review and validation of the rating systems to ensure continuing compliance with the applicable HKMA requirements; (ii)  clearly delineate and assign the responsibilities and accountabilities   for   the   effective   operations   and maintenance of the rating systems to the respective business, credit, finance, IT, operations and other support or control functions, or personnel; (iii) ensure that adequate training on the rating systems is provided for staff in the relevant business, credit, finance, IT, operations and other support or control functions; (iv) make necessary changes to the existing policies and procedures as well as systems and controls in order to integrate the use of the rating systems into the AI’s credit risk management processes and culture; (v) ensure that the rating systems are put to use properly; (vi) ensure that the usage of the rating systems extends beyond purely regulatory capital reporting to decision-making and monitoring processes including credit approval, limits setting, credit monitoring and reporting, pricing, internal capital allocation, provisioning etc.(see  subsection 5.4);\n\n### Section 4.8: As regards the applicable HKMA requirements for quarterly review of the performance and predictive ability of the rating systems, the HKMA recognizes that an increase in the number of defaulted cases over a three-month period may not be significant, especially for certain portfolios with low frequency of default events.  In this case, it will be sufficient for senior management to examine only the default and rating migration statistics in the quarterly review exercise, provided that the AI is able to justify its approach with empirical evidence.  In addition, the quarterly review of the default and rating migration statistics should include comparisons with expectations and historical figures.\nAs regards the applicable HKMA requirements for quarterly review of the performance and predictive ability of the rating systems, the HKMA recognizes that an increase in the number of defaulted cases over a three-month period may not be significant, especially for certain portfolios with low frequency of default events.  In this case, it will be sufficient for senior management to examine only the default and rating migration statistics in the quarterly review exercise, provided that the AI is able to justify its approach with empirical evidence.  In addition, the quarterly review of the default and rating migration statistics should include comparisons with expectations and historical figures.\n\n### Section 4.9: Information on the internal ratings should be reported to the Board and senior management regularly.  The depth and frequency of reporting may vary with the significance and type of information, and the oversight responsibilities of the recipients.  The reports should, at a minimum, cover the following information: (i)   risk profile of the AI’s obligors by grade(or pool for retail exposures);\nInformation on the internal ratings should be reported to the Board and senior management regularly.  The depth and frequency of reporting may vary with the significance and type of information, and the oversight responsibilities of the recipients.  The reports should, at a minimum, cover the following information: (i)   risk profile of the AI’s obligors by grade(or pool for retail exposures);",
  "19": "### Section 5.1: Independence Independent credit risk control\nIndependence Independent credit risk control\n\n5.1.1: In relation to the minimum requirements on an AI’s credit risk control unit set out in §1(c) of Schedule 2 to the BCR\n\n5.1.2: An independent rating approval process is where the parties11 responsible for approving ratings and transactions are separate from those responsible for\n\n5.1.3: Rating processes vary by AI and by portfolio but generally involve a rating “assignor” and a rating “approver”.  For a judgement-based rating system, the HKMA expects that credit officers should normally be the party responsible for approving ratings.  Their independence should be safeguarded  through  independent  and  separate functional reporting lines, and well-defined performance measures(e.g. adherence to policy, rating accuracy and timeliness).\n\n5.1.4: In some cases, ratings are assigned and approved within sales and marketing by staff(although at perhaps different levels of seniority) whose compensation is tied to the volume of business they generate.  The HKMA does not normally consider that such arrangements can achieve an adequate degree of independence in the rating approval process.  However, the HKMA may, in both the initial IRB recognition process and the ongoing review process of the rating systems, take into account the size and nature of the portfolio to which these arrangements are applied, and the compensating controls in place to mitigate the inherent conflict of interest(such as restrained credit limits, independent post-approval review of ratings, and more frequent internal audit coverage, to prevent any bias in the rating assignment and approval processes).\n\n5.1.5: The above requirements are primarily intended to apply to cases where expert judgement forms part of the inputs to the rating assignment or approval processes.  If the rating assignment and approval processes are highly automated and all the rating criteria are based on objective factors(i.e. expert judgement does not form any part of the rating process), the independent review should at a minimum include a process for verifying the accuracy and completeness of the data inputs.\n\n5.1.6: To ensure the integrity of rating systems(including risk quantification), AIs should have a comprehensive and independent validation process.  The unit(s) responsible for validation should be functionally independent of the staff and management functions responsible for developing the underlying rating systems and performing risk quantification activities, and have sufficient stature in the organisational hierarchy to challenge effectively the work of the rating system developers.  The activities of this validation process may be distributed across multiple functions or housed within one unit.  AIs may choose a structure that fits their management and oversight framework.  However, to maintain the independence of the validation process, cross-validations, whereby two or more separate units validate the rating systems developed by one another, should be avoided. Individuals performing the validation should possess the requisite technical skills and expertise.  The validation of rating systems should encompass the following aspects: (i)   compliance with the applicable HKMA requirements; (ii)  compliance with the AIs’ established policies and procedures; (iii) quantification process and accuracy of the credit risk component estimates12; (iv) rating system development and usage13;\n\n5.1.7: The independent validation unit(s) should formulate a plan to define the validation activities and review processes to be performed.  The plan should be modified as appropriate having regard to findings identified in the validation processes.  The independent validation unit(s) should perform its own tests of all material aspects of the rating systems, including their performance, quality of databases used, and data cleansing.  These tests should also cover those already performed by the model developers to check their reliability.\n\n5.1.8: The validation processes should seek to identify any weaknesses, make recommendations and ensure that corrective actions are taken accordingly.  Significant findings identified from the validation processes must be reported to the Board and senior management.\n\n5.1.9: AIs that at present lack sufficient in-house expertise to be able to perform the validation function adequately should make appropriate use of external support that is independent and has relevant knowledge and experience. Those AIs that already have the needed skills and resources in-house should nonetheless consider the benefits of supplementing their internal processes with external reviews.  External reviewers are likely to possess a broader perspective on the use of rating systems in different jurisdictions and in different institutions, and they may possess more comprehensive data sets to support the cross-testing of rating systems.  Notwithstanding that some validation activities are outsourced to external parties, AIs’ internal independent validation unit(s) should retain full and ultimate responsibility for the validation activities and results.\n\n### Section 5.2: Transparency\nTransparency\n\n5.2.1: AIs’ rating systems should be transparent to enable third parties, such as rating system reviewers, internal or external auditors, and the HKMA, to understand the design, operations and accuracy of the rating systems, and to evaluate whether the systems are performing as intended.  Transparency is an ongoing requirement and should    be    achieved   through   comprehensive documentation with regular and timely reviews, and updates as appropriate(e.g. as and when modifications are made to the rating systems).  This underlies the minimum requirements stipulated in §1(e) of Schedule 2to the BCR.\n\n5.2.2: An AI should document in writing the design of its rating systems and related operations as evidence of its compliance with the applicable HKMA requirements.\n\n5.2.3: The AI’s documentation should provide a description of the overarching design of the rating systems, including: (i)   the purpose of the rating systems; (ii)  portfolio differentiation; and (iii) the rating approach(i.e. how quickly ratings are expected to migrate in response to economic cycles) and implications for the AI’s capital planning process14.\n\n5.2.4: Rating criteria and definitions should be clearly documented. These include: (i)   the relationship between obligor grades(or pools for retail exposures) in terms of the level of risk each grade(or pool) implies, and the risk of each grade(or pool) in terms of both a description of the probability of default typical for obligors assigned the grade(or pool) and the criteria used to\n\n5.2.5: Documentation of the rating process and rating system operations should include the following: (i)   the organisation of rating assignment/approval; (ii)  responsibilities of parties that rate obligors and facilities; (iii) parties that have the authority to approve ratings and that have the authority to approve exceptions(including overrides); (iv) situations where exceptions and overrides can be approved and the procedures for such approval; (v) the rating criteria and procedures(including the frequency of rating reviews); (vi) the process and procedures for updating obligor and facility information; and (vii) the rationale and criteria for assigning obligors(or facilities) to a particular rating system if multiple rating systems are used.\n\n5.2.6: In respect of internal control structure, the documentation should be able to demonstrate that: (i)   the Board and senior management have adequate oversight of the rating systems and rating process; (ii)  independence of the rating assignment/approval process is achieved and ensured; (iii) there are proper audit trails on the history of major changes to the rating process and criteria, in particular to support identification of changes made to the rating process and criteria subsequent to the last supervisory review15; and (iv) there are established procedures(including frequency, parties responsible and reporting of results) and performance standards for reviewing the rating systems in respect of rating accuracy, rating criteria and rating processes in order to determine whether they remain fully applicable to the current portfolio and to external conditions, and that these procedures are adhered to.\n\n5.2.7: Where judgement is used in the rating process, how personal experience and subjective assessment are deployed is less transparent.  AIs should offset this shortcoming by applying greater independence in the rating approval process and an enhanced rating system review.\n\n5.2.8: Where an AI employs a statistical model in the rating process and/or estimation of the credit risk components, its documentation should include: (i)   the theory, assumptions and/or mathematical and empirical   basis   of    the   assignment   of obligors/facilities to grades(or pools for retail exposures) and estimation of the associated credit risk components, and the sources of data used to develop the model;\n\n5.2.9: Use of a model obtained from a third-party vendor that claims proprietary technology is not a justification for exemption from documentation or any other applicable HKMA requirements.  The burden is on the vendor and the AI to satisfy the applicable HKMA requirements.\n\n### Section 5.3: Accountability\nAccountability\n\n5.3.1: To ensure proper accountability, AIs should have policies that specify individuals or parties responsible for rating accuracy and rating system performance, and establish performance standards in relation to their responsibilities.\n\n5.3.2: The responsibilities(including lines of reporting and the authority of individuals) must be specific and clearly defined.  The performance standards should be measurable against specific objectives, with incentive compensation tied to these standards.\n\n5.3.3: For example, performance measures of personnel responsible for rating assignment may include number and frequency of rating errors, significance of errors(e.g. multiple downgrades), and proper and consistent application of criteria, including override criteria.\n\n5.3.4: Staff who assign and approve ratings, derive the credit risk component estimates, or oversee rating systems must be held accountable for complying with rating system policies and ensuring that those aspects of the rating systems under their control are unbiased and accurate.  For accountability to be effective, these staff must have the knowledge and skills, and tools and resources necessary to discharge their responsibilities.\n\n5.3.5: If AIs use models in the rating assignment process, a mechanism should be in place to maintain an up-to-date inventory of models16, and an accountability chart of the roles of the parties within the AIs responsible for every aspect of the models including the design, development, use, data updating, data checking, and validation of the models.\n\n5.3.6: A specific individual at sufficiently senior level should have the responsibility for the overall performance of the rating systems.  This individual must ensure that the rating systems and all of their components(rating assignments, estimation of the credit risk components, data collection, control and oversight mechanisms etc.) are functioning as intended.  When these components are distributed across multiple units of the AI, this individual should be responsible for ensuring that the parts work together effectively and efficiently.\n\n### Section 5.4: Use of rating systems Areas of use\nUse of rating systems Areas of use\n\n5.4.1: In relation to the minimum requirements set out in§(1)(b)(v) and(vi) and §2(b) of Schedule 2 to the BCR, an AI should be able to demonstrate that its rating systems from which ratings and estimates of the credit risk components are generated for regulatory capital calculation are used in such a way as to exert a direct and observable influence on the AI’s decision-making and actions.  It is not acceptable if an AI’s rating systems and estimates are designed and implemented exclusively for the purpose of regulatory capital calculation.\n\n5.4.2: In particular, the HKMA expects the AI to apply its internal ratings and estimates of the credit risk components for internal decision-making purposes for at least three years 17, covering credit approval, credit monitoring, reporting of credit risk information to the AI’s Board of Directors and senior management, and the majority of the following areas: (i)   pricing; (ii)  setting of limits for individual exposures and portfolios; (iii) determining provisioning; (iv) modelling and management of economic capital; (v) assessment of total capital requirements in relation to credit risks under the AI’s Capital Adequacy Assessment Process(“CAAP”); (vi) stress testing; (vii) assessment of risk appetite; (viii) formulating business strategies(e.g. acquisition strategy for new exposures and collection strategy for problem loans); (ix) setting of, and assessment against, profitability and performance targets;\n\n5.4.3: AIs may not necessarily use exactly the same ratings and estimates for both regulatory capital calculation and internal purposes.  Where there are differences, however, AIs should document the differences and their justifications.  The justifications should include: (i)   a demonstration of consistency amongst the risk factors and rating criteria used in generating the credit risk component estimates for regulatory capital calculation and those for internal purposes; (ii)  a demonstration of consistency amongst the ratings and estimates used in regulatory capital calculation and those for internal purposes; and (iii) qualitative and quantitative analyses of the logic and rationale for the differences.\n\n5.4.4: The justifications should be reviewed by the credit risk control unit and approved by senior management.  The document detailing the differences and the justifications should be provided to the HKMA for review upon request.\n\n5.4.5: The HKMA notes that some AIs may maintain more than one rating system for the same portfolio.  For example, one system might be used for the purpose of calculating regulatory capital and another for the purpose of benchmarking.  These rating systems may all have been developed in-house, or obtained from external sources, or a combination of both.  In all such cases, the HKMA expects an AI to provide documented justification for its application of a specific rating system for a specific purpose, and for the role it has assigned to that system in its credit management process.  In its assessment of whether the “use test” for rating systems has been met, the HKMA will consider the extent to which an AI makes internal use of all the rating systems as a whole, rather than applying the test on an individual system basis.\n\n### Section 5.5: 1 Internal audit function should review at least annually an AI’s rating systems and their operations(including the validation process and the estimation of the credit risk components) and the operations of the credit function and its related credit risk control unit.  The purpose is to verify whether the control mechanisms over the rating systems are effective, adequate and functioning as intended and the AI is in compliance with the applicable HKMA requirements.  The internal audit function should document the findings and report them to the Board and senior management.\n1 Internal audit function should review at least annually an AI’s rating systems and their operations(including the validation process and the estimation of the credit risk components) and the operations of the credit function and its related credit risk control unit.  The purpose is to verify whether the control mechanisms over the rating systems are effective, adequate and functioning as intended and the AI is in compliance with the applicable HKMA requirements.  The internal audit function should document the findings and report them to the Board and senior management.\n\n5.5.2: The areas of review should include the independence of the credit risk control unit, the depth, scope and quality of work conducted by it in respect of the AI’s use of the IRB approach, as well as the actions taken by AI to address deficiencies identified from the validation of the rating systems.\n\n5.5.3: The internal audit function should give an opinion on: (i)    the continuing appropriateness, relevance and comprehensiveness of the existing control mechanisms; (ii)   the adequacy of expertise of staff responsible for the operations of the credit risk control unit; (iii)   the resources available to these staff; and\n\n5.5.4: In respect of the adequacy of the internal audit function, the AI should be able to demonstrate to the HKMA that in particular: (i)   the internal audit staff are equipped with the required skill sets and are provided with relevant resources adequately; and (ii) the audit programme is comprehensive, and the assessment of compliance with the applicable HKMA requirements is covered in the annual audit plan.\n\n5.5.5: Under the IRB recognition process, AIs are required to submit self-assessment questionnaires and relevant supporting documents for review by the HKMA.  The HKMA expects internal audit function to be one of the parties signing off on the completed self-assessment as evidence that it has verified an AI’s adherence to all the applicable HKMA requirements. External audit\n\n5.5.6: As part of the process of certifying financial statements, external auditors should gain comfort from an AI that its rating systems are measuring credit risk appropriately and that its regulatory capital position is fairly presented. External auditors should also seek to assure themselves that the AI’s internal controls relating to the calculation of regulatory capital are compliant with the applicable HKMA requirements.",
  "20": "### Section 3.5.4: 3.5 Subsection\nAIs may use the quantitative validation methods cited in this module or other methods for performing the above analyses.  For the latter, the AIs should be able to demonstrate to the HKMA that the techniques are theoretically  sound,   well-documented,  consistently applied and able to meet the standards applicable to the generally accepted quantitative techniques.  The AIs should be able to provide the rationale for choosing the techniques and demonstrate the appropriateness of using such techniques.\n\n### Section 5.6: Treatment of third-party vendor rating systems19\nTreatment of third-party vendor rating systems19\n\n5.6.1: AIs commonly make use of outside expertise to develop rating systems for decision-making or risk management purposes.  In the context of the IRB approach, a third-party vendor rating system(“vendor system”) is a rating system developed by a third party vendor and used by an AI to assign its credit risk exposures to rating grades(or pools for retail exposures) or to estimate the credit risk components of its exposures.\n\n5.6.2: The use of a rating system obtained from a third-party vendor that claims proprietary technology is not a justification for exemption from documentation or any other applicable HKMA requirements in respect of the rating system.  Thus, these systems generally have to fulfil the same applicable HKMA requirements as rating systems produced in-house.  In addition, senior management should ensure that the outsourced activities performed by third-party vendors are supported by sufficient quality control measures to ensure that the applicable HKMA requirements are met on a continuous basis.  AIs may refer to SA-2 “Outsourcing” for further guidance.\n\n5.6.3: The burden is on the AI to satisfy the HKMA that it complies with these applicable HKMA requirements.  The HKMA’s assessment regarding a vendor system will focus on the transparency of the system and on its linkage to the AI’s internal information used in the rating process. Where the HKMA considers appropriate, it may request the AI and the third-party vendor to provide detailed information for the HKMA’s assessment.\n\n5.6.4: AIs should demonstrate that they have the in-house knowledge to understand the key aspects of the vendor systems.  In particular, they should be able to demonstrate a good understanding of the development(e.g. the overarching design, assumptions, data used, methods and criteria for risk factor selection and determination of the associated weights) and the appropriate use of vendor systems.  This requires third-party vendors to document the development of the systems and the fundamentals of their validation processes in a way that permits other parties to understand the methodologies applied, and to assess whether the systems perform adequately on the AI’s current portfolios.  AIs should identify and consider in the course of monitoring their systems all the limitations of the systems and the circumstances in which the systems do not perform as expected.\n\n5.6.5: Where AIs make use of vendor systems, they should ensure that they possess sufficient in-house expertise to support and assess these systems.  Staff who are vendor system users should be provided with adequate training in the use of these systems.\n\n5.6.6: Where parts of a vendor system are used simultaneously with parts developed in-house in the rating process, AIs need to be clear about the nature and content of the information(data) that is processed in the vendor system. They should ensure that this information is appropriately linked to information that is processed by the parts developed in-house, so that the aggregation of the different parts of the system does not result in an inconsistent rating method.\n\n### Section 5.7: Treatment of group-wide rating systems\nTreatment of group-wide rating systems\n\n5.7.1: In relation to §9(1)(d) of the BCR on an AI’s use of a group-wide rating system, the AI is expected to demonstrate that the system is suitable for calculating the credit risk of its exposures for regulatory capital reporting.  The AI should assess that the data used and assumptions adopted for developing the rating system are relevant to its exposures, and that the system performs satisfactorily on its exposures in respect of both risk differentiation and accuracy of the credit risk components.  The AI should conduct these assessments on an ongoing basis as part of its regular performance monitoring and independent validation.\n\n5.7.2: An AI’s rating system(or part of the rating system) may be centrally developed and monitored on a group basis.  In assessing whether the AI meet the applicable HKMA"
}